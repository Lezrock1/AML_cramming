arch:
  architectures:
  - ScriptableCrammedBERT
  num_transformer_layers: 16
  hidden_size: 768
  intermed_size: 3072
  hidden_dropout_prob: 0.1
  norm: LayerNorm
  norm_eps: 1.0e-12
  norm_scheme: pre
  nonlin: GELUglu
  tie_weights: true
  decoder_bias: false
  sparse_prediction: ${train.objective.mlm_probability}
  loss: cross-entropy
  objective_layout: MLM
  embedding:
    vocab_size: null
    pos_embedding: scaled-sinusoidal
    dropout_prob: 0.1
    pad_token_id: 0
    max_seq_length: 128
    embedding_dim: ${arch.hidden_size}
    normalization: true
    stable_low_precision: false
  attention:
    type: self-attention
    causal_attention: false
    num_attention_heads: 12
    dropout_prob: 0.1
    skip_output_projection: false
    qkv_bias: false
    rotary_embedding: false
    seq_op_in_fp32: false
    sequence_op: torch-softmax
  init:
    type: normal
    std: 0.02
  ffn_layer_frequency: 1
  skip_head_transform: true
  use_bias: false
  final_norm: true
  num_labels: null
  classification_head:
    pooler: avg
    include_ff_layer: true
    head_dim: 1024
    nonlin: Tanh
    classifier_dropout: ${arch.hidden_dropout_prob}
data:
  name: pile-readymade
  sources:
    hub:
      provider: hub
  hf_location: JonasGeiping/the_pile_WordPiecex32768_2efdb9d060d1ae95faf952ec1a50f020
  streaming: true
  vocab_size: 32768
  seq_length: 128
impl:
  path: data
  local_staging_dir: null
  forbid_dataset_preprocessing: false
  temporary_corpus: false
  max_raw_chunk_size: 8000000.0
  print_loss_every_nth_step: 1000
  save_intermediate_checkpoints: false
  save_every_nth_step: 10000
  resume_run_after_preempt: false
  troubleshoot_strategy: none
  early_termination:
    enabled: false
    budget: 3
    loss_threshold: 6.0
  microbatch_size: 8
  threads: 8
  benchmark: true
  deterministic: false
  non_blocking: true
  tf32_allowed: true
  matmul_precision: high
  pad_to_multiple_of: 8
  shuffle_in_dataloader: false
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  default_precision: float
  dist_backend: nccl
  sharing_strategy: null
  enable_huggingface_offline_mode: false
  local_rank: null
  push_to_huggingface_hub: false
  hf_directoy_name: test-crammedBERT-c5
  add_env_variables: null
  name: torch-default
  mixed_precision: true
  grad_scaling: true
  mixed_precision_target_dtype: float16
  zero_redundancy_optimizer: false
  broadcast_buffers: false
  bucket_cap_mb: 25
  gradient_as_bucket_view: true
  static_graph: false
  foreach_optimizer: false
  compile_torch: false
  mode: null
  dynamic: false
  fullgraph: false
  backend: inductor
  _inductor_vars:
    max_autotune_gemm: true
    max_autotune_pointwise: false
    triton:
      cudagraphs: true
      cudagraph_trees: false
    permute_fusion: true
    shape_padding: true
  enable_mem_efficient_sdp: true
  enable_math_sdp: true
  enable_flash_sdp: true
wandb:
  enabled: false
  entity: null
  project: null
  tags: []
train:
  optim:
    type: AdamW
    lr: 0.001
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-12
    weight_decay: 0.01
    amsgrad: false
    fused: null
  optim_mod:
    name: none
  name: bert-o4
  limited_decay_keys:
  - bias
  - LayerNorm.bias
  - LayerNorm.weight
  - norm
  warmup_steps: 0
  cooldown_steps: 0
  steps: 900000
  scheduler: budget-triangle2
  batch_size: 8192
  batch_size_ramp: 0.6
  gradient_clipping: 0.5
  pretrain_in_train_mode: false
  objective:
    name: masked-lm
    mlm_probability: 0.25
    use_80_20_rule: true
    disable_mlm: false
    token_drop: 0.0
  reverse_dataset_order: false
  budget: ${budget}
base_dir: outputs
seed: null
name: smoke_pile_train
budget: 0.02
dryrun: false
