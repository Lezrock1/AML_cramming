/home/group.kurse/jnoow006/.conda/envs/cram/lib/python3.11/site-packages/torch/cuda/__init__.py:54: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Generating train split:   0%|          | 0/8551 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 8551/8551 [00:00<00:00, 860001.28 examples/s]
Generating validation split:   0%|          | 0/1043 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 1043/1043 [00:00<00:00, 554878.12 examples/s]
Generating test split:   0%|          | 0/1063 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1063/1063 [00:00<00:00, 617783.73 examples/s]
Running tokenizer on dataset:   0%|          | 0/8551 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 8551/8551 [00:00<00:00, 87220.26 examples/s]
Running tokenizer on dataset:   0%|          | 0/1043 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 1043/1043 [00:00<00:00, 66750.98 examples/s]
Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]Generating train split:  62%|██████▏   | 242000/392702 [00:00<00:00, 2408637.61 examples/s]Generating train split: 100%|██████████| 392702/392702 [00:00<00:00, 2447300.53 examples/s]
Generating validation_matched split:   0%|          | 0/9815 [00:00<?, ? examples/s]Generating validation_matched split: 100%|██████████| 9815/9815 [00:00<00:00, 1651771.21 examples/s]
Generating validation_mismatched split:   0%|          | 0/9832 [00:00<?, ? examples/s]Generating validation_mismatched split: 100%|██████████| 9832/9832 [00:00<00:00, 1974640.73 examples/s]
Generating test_matched split:   0%|          | 0/9796 [00:00<?, ? examples/s]Generating test_matched split: 100%|██████████| 9796/9796 [00:00<00:00, 1916211.27 examples/s]
Generating test_mismatched split:   0%|          | 0/9847 [00:00<?, ? examples/s]Generating test_mismatched split: 100%|██████████| 9847/9847 [00:00<00:00, 1791347.65 examples/s]
Running tokenizer on dataset:   0%|          | 0/392702 [00:00<?, ? examples/s]Running tokenizer on dataset:   1%|          | 2048/392702 [00:00<00:29, 13424.00 examples/s]Running tokenizer on dataset:   2%|▏         | 7168/392702 [00:00<00:13, 28624.21 examples/s]Running tokenizer on dataset:   3%|▎         | 12288/392702 [00:00<00:11, 33515.54 examples/s]Running tokenizer on dataset:   4%|▍         | 16384/392702 [00:00<00:14, 26296.94 examples/s]Running tokenizer on dataset:   5%|▌         | 21504/392702 [00:00<00:12, 30556.70 examples/s]Running tokenizer on dataset:   7%|▋         | 26624/392702 [00:00<00:10, 33972.75 examples/s]Running tokenizer on dataset:   8%|▊         | 32768/392702 [00:01<00:09, 36405.41 examples/s]Running tokenizer on dataset:   9%|▉         | 36864/392702 [00:01<00:11, 30585.01 examples/s]Running tokenizer on dataset:  11%|█         | 41984/392702 [00:01<00:10, 33438.25 examples/s]Running tokenizer on dataset:  12%|█▏        | 47104/392702 [00:01<00:09, 35336.50 examples/s]Running tokenizer on dataset:  13%|█▎        | 52224/392702 [00:01<00:11, 30279.27 examples/s]Running tokenizer on dataset:  15%|█▍        | 58368/392702 [00:01<00:09, 33551.49 examples/s]Running tokenizer on dataset:  16%|█▌        | 63488/392702 [00:02<00:11, 29551.78 examples/s]Running tokenizer on dataset:  17%|█▋        | 67584/392702 [00:02<00:10, 31544.80 examples/s]Running tokenizer on dataset:  19%|█▉        | 73728/392702 [00:02<00:09, 34792.00 examples/s]Running tokenizer on dataset:  20%|██        | 79872/392702 [00:02<00:10, 30185.51 examples/s]Running tokenizer on dataset:  22%|██▏       | 84992/392702 [00:02<00:09, 32623.82 examples/s]Running tokenizer on dataset:  23%|██▎       | 90112/392702 [00:02<00:08, 34688.55 examples/s]Running tokenizer on dataset:  24%|██▍       | 94208/392702 [00:02<00:08, 35616.53 examples/s]Running tokenizer on dataset:  25%|██▌       | 98304/392702 [00:03<00:09, 30192.49 examples/s]Running tokenizer on dataset:  26%|██▋       | 103424/392702 [00:03<00:08, 32829.00 examples/s]Running tokenizer on dataset:  28%|██▊       | 109568/392702 [00:03<00:07, 35782.19 examples/s]Running tokenizer on dataset:  29%|██▉       | 113664/392702 [00:03<00:09, 30402.74 examples/s]Running tokenizer on dataset:  31%|███       | 119808/392702 [00:03<00:08, 33789.64 examples/s]Running tokenizer on dataset:  32%|███▏      | 124928/392702 [00:03<00:07, 35476.87 examples/s]Running tokenizer on dataset:  33%|███▎      | 129024/392702 [00:04<00:08, 30330.24 examples/s]Running tokenizer on dataset:  34%|███▍      | 134144/392702 [00:04<00:07, 33012.89 examples/s]Running tokenizer on dataset:  35%|███▌      | 139264/392702 [00:04<00:07, 35332.69 examples/s]Running tokenizer on dataset:  37%|███▋      | 145408/392702 [00:04<00:07, 31041.53 examples/s]Running tokenizer on dataset:  38%|███▊      | 149504/392702 [00:04<00:07, 32711.27 examples/s]Running tokenizer on dataset:  40%|███▉      | 155648/392702 [00:04<00:06, 35578.68 examples/s]Running tokenizer on dataset:  41%|████      | 160768/392702 [00:05<00:07, 30271.97 examples/s]Running tokenizer on dataset:  43%|████▎     | 166912/392702 [00:05<00:06, 33466.78 examples/s]Running tokenizer on dataset:  44%|████▎     | 171008/392702 [00:05<00:06, 34503.44 examples/s]Running tokenizer on dataset:  45%|████▍     | 175104/392702 [00:05<00:07, 29485.80 examples/s]Running tokenizer on dataset:  46%|████▌     | 179200/392702 [00:05<00:06, 31095.99 examples/s]Running tokenizer on dataset:  47%|████▋     | 184320/392702 [00:05<00:06, 33751.77 examples/s]Running tokenizer on dataset:  48%|████▊     | 188416/392702 [00:05<00:06, 29206.32 examples/s]Running tokenizer on dataset:  50%|████▉     | 194560/392702 [00:06<00:06, 32749.66 examples/s]Running tokenizer on dataset:  51%|█████     | 198656/392702 [00:06<00:05, 34397.06 examples/s]Running tokenizer on dataset:  52%|█████▏    | 202752/392702 [00:06<00:05, 35836.13 examples/s]Running tokenizer on dataset:  53%|█████▎    | 206848/392702 [00:06<00:06, 29965.18 examples/s]Running tokenizer on dataset:  54%|█████▎    | 210944/392702 [00:06<00:05, 32377.32 examples/s]Running tokenizer on dataset:  55%|█████▌    | 216064/392702 [00:06<00:05, 34526.50 examples/s]Running tokenizer on dataset:  56%|█████▋    | 221184/392702 [00:06<00:05, 29847.22 examples/s]Running tokenizer on dataset:  57%|█████▋    | 225280/392702 [00:06<00:05, 31971.05 examples/s]Running tokenizer on dataset:  58%|█████▊    | 229376/392702 [00:07<00:04, 33672.05 examples/s]Running tokenizer on dataset:  60%|█████▉    | 234496/392702 [00:07<00:05, 29255.95 examples/s]Running tokenizer on dataset:  61%|██████    | 239616/392702 [00:07<00:04, 32347.40 examples/s]Running tokenizer on dataset:  63%|██████▎   | 245760/392702 [00:07<00:04, 35404.04 examples/s]Running tokenizer on dataset:  64%|██████▍   | 250880/392702 [00:07<00:04, 30601.86 examples/s]Running tokenizer on dataset:  65%|██████▌   | 256000/392702 [00:07<00:04, 33123.08 examples/s]Running tokenizer on dataset:  66%|██████▋   | 261120/392702 [00:08<00:03, 34789.92 examples/s]Running tokenizer on dataset:  68%|██████▊   | 266240/392702 [00:08<00:04, 30317.01 examples/s]Running tokenizer on dataset:  69%|██████▉   | 272384/392702 [00:08<00:03, 33404.10 examples/s]Running tokenizer on dataset:  71%|███████   | 277504/392702 [00:08<00:03, 35339.78 examples/s]Running tokenizer on dataset:  72%|███████▏  | 283648/392702 [00:08<00:03, 30979.61 examples/s]Running tokenizer on dataset:  74%|███████▍  | 289792/392702 [00:08<00:03, 33749.58 examples/s]Running tokenizer on dataset:  75%|███████▌  | 294912/392702 [00:09<00:02, 35505.25 examples/s]Running tokenizer on dataset:  76%|███████▋  | 300032/392702 [00:09<00:02, 30917.67 examples/s]Running tokenizer on dataset:  78%|███████▊  | 306176/392702 [00:09<00:02, 33846.93 examples/s]Running tokenizer on dataset:  80%|███████▉  | 312320/392702 [00:09<00:02, 30723.05 examples/s]Running tokenizer on dataset:  81%|████████  | 317440/392702 [00:09<00:02, 33056.61 examples/s]Running tokenizer on dataset:  82%|████████▏ | 321536/392702 [00:09<00:02, 34342.88 examples/s]Running tokenizer on dataset:  83%|████████▎ | 326656/392702 [00:10<00:01, 35885.18 examples/s]Running tokenizer on dataset:  84%|████████▍ | 330752/392702 [00:10<00:02, 30421.15 examples/s]Running tokenizer on dataset:  86%|████████▌ | 336896/392702 [00:10<00:01, 33847.64 examples/s]Running tokenizer on dataset:  87%|████████▋ | 340992/392702 [00:10<00:01, 34981.66 examples/s]Running tokenizer on dataset:  88%|████████▊ | 345088/392702 [00:10<00:01, 29616.18 examples/s]Running tokenizer on dataset:  89%|████████▉ | 350208/392702 [00:10<00:01, 32305.66 examples/s]Running tokenizer on dataset:  90%|█████████ | 355328/392702 [00:10<00:01, 34626.70 examples/s]Running tokenizer on dataset:  92%|█████████▏| 359424/392702 [00:11<00:01, 29412.19 examples/s]Running tokenizer on dataset:  93%|█████████▎| 364544/392702 [00:11<00:00, 32418.33 examples/s]Running tokenizer on dataset:  94%|█████████▍| 369664/392702 [00:11<00:00, 34804.89 examples/s]Running tokenizer on dataset:  95%|█████████▌| 373760/392702 [00:11<00:00, 35818.73 examples/s]Running tokenizer on dataset:  96%|█████████▌| 377856/392702 [00:11<00:00, 29842.86 examples/s]Running tokenizer on dataset:  97%|█████████▋| 381952/392702 [00:11<00:00, 32059.68 examples/s]Running tokenizer on dataset:  99%|█████████▊| 387072/392702 [00:11<00:00, 34315.26 examples/s]Running tokenizer on dataset: 100%|█████████▉| 391168/392702 [00:12<00:00, 28864.32 examples/s]Running tokenizer on dataset: 100%|██████████| 392702/392702 [00:12<00:00, 32352.48 examples/s]
Running tokenizer on dataset:   0%|          | 0/9815 [00:00<?, ? examples/s]Running tokenizer on dataset:  63%|██████▎   | 6144/9815 [00:00<00:00, 42820.92 examples/s]Running tokenizer on dataset: 100%|██████████| 9815/9815 [00:00<00:00, 41873.54 examples/s]
Running tokenizer on dataset:   0%|          | 0/9832 [00:00<?, ? examples/s]Running tokenizer on dataset:  21%|██        | 2048/9832 [00:00<00:00, 14224.78 examples/s]Running tokenizer on dataset:  73%|███████▎  | 7168/9832 [00:00<00:00, 29127.38 examples/s]Running tokenizer on dataset: 100%|██████████| 9832/9832 [00:00<00:00, 29091.09 examples/s]
Running tokenizer on dataset:   0%|          | 0/9796 [00:00<?, ? examples/s]Running tokenizer on dataset:  63%|██████▎   | 6144/9796 [00:00<00:00, 42329.82 examples/s]Running tokenizer on dataset: 100%|██████████| 9796/9796 [00:00<00:00, 42214.14 examples/s]
Running tokenizer on dataset:   0%|          | 0/9847 [00:00<?, ? examples/s]Running tokenizer on dataset:  31%|███       | 3072/9847 [00:00<00:00, 18137.89 examples/s]Running tokenizer on dataset:  94%|█████████▎| 9216/9847 [00:00<00:00, 31335.55 examples/s]Running tokenizer on dataset: 100%|██████████| 9847/9847 [00:00<00:00, 29587.23 examples/s]
Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 855989.93 examples/s]
Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 279803.14 examples/s]
Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 684177.25 examples/s]
Running tokenizer on dataset:   0%|          | 0/3668 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 34259.96 examples/s]Running tokenizer on dataset: 100%|██████████| 3668/3668 [00:00<00:00, 33768.98 examples/s]
Running tokenizer on dataset:   0%|          | 0/408 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 408/408 [00:00<00:00, 28494.20 examples/s]
Generating train split:   0%|          | 0/104743 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 104743/104743 [00:00<00:00, 1926791.97 examples/s]
Generating validation split:   0%|          | 0/5463 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 5463/5463 [00:00<00:00, 1223880.07 examples/s]
Generating test split:   0%|          | 0/5463 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 5463/5463 [00:00<00:00, 1268882.64 examples/s]
Running tokenizer on dataset:   0%|          | 0/104743 [00:00<?, ? examples/s]Running tokenizer on dataset:   2%|▏         | 2048/104743 [00:00<00:07, 13296.02 examples/s]Running tokenizer on dataset:   6%|▌         | 6144/104743 [00:00<00:03, 24857.93 examples/s]Running tokenizer on dataset:  10%|▉         | 10240/104743 [00:00<00:03, 29754.01 examples/s]Running tokenizer on dataset:  14%|█▎        | 14336/104743 [00:00<00:02, 32052.59 examples/s]Running tokenizer on dataset:  18%|█▊        | 18432/104743 [00:00<00:03, 25631.56 examples/s]Running tokenizer on dataset:  22%|██▏       | 22528/104743 [00:00<00:02, 28542.25 examples/s]Running tokenizer on dataset:  25%|██▌       | 26624/104743 [00:00<00:02, 30596.13 examples/s]Running tokenizer on dataset:  29%|██▉       | 30720/104743 [00:01<00:02, 32152.04 examples/s]Running tokenizer on dataset:  33%|███▎      | 34816/104743 [00:01<00:02, 26501.06 examples/s]Running tokenizer on dataset:  37%|███▋      | 38912/104743 [00:01<00:02, 28791.41 examples/s]Running tokenizer on dataset:  41%|████      | 43008/104743 [00:01<00:02, 30684.57 examples/s]Running tokenizer on dataset:  45%|████▍     | 47104/104743 [00:01<00:01, 31855.05 examples/s]Running tokenizer on dataset:  50%|████▉     | 52224/104743 [00:01<00:01, 26905.67 examples/s]Running tokenizer on dataset:  54%|█████▍    | 56320/104743 [00:01<00:01, 28942.50 examples/s]Running tokenizer on dataset:  58%|█████▊    | 60416/104743 [00:02<00:01, 30432.31 examples/s]Running tokenizer on dataset:  63%|██████▎   | 65536/104743 [00:02<00:01, 26102.03 examples/s]Running tokenizer on dataset:  66%|██████▋   | 69632/104743 [00:02<00:01, 28010.29 examples/s]Running tokenizer on dataset:  70%|███████   | 73728/104743 [00:02<00:01, 29713.87 examples/s]Running tokenizer on dataset:  74%|███████▍  | 77824/104743 [00:02<00:00, 31245.07 examples/s]Running tokenizer on dataset:  79%|███████▉  | 82944/104743 [00:02<00:00, 26922.77 examples/s]Running tokenizer on dataset:  83%|████████▎ | 87040/104743 [00:03<00:00, 28896.76 examples/s]Running tokenizer on dataset:  87%|████████▋ | 91136/104743 [00:03<00:00, 30530.63 examples/s]Running tokenizer on dataset:  91%|█████████ | 95232/104743 [00:03<00:00, 25778.28 examples/s]Running tokenizer on dataset:  95%|█████████▍| 99328/104743 [00:03<00:00, 27724.40 examples/s]Running tokenizer on dataset:  99%|█████████▊| 103424/104743 [00:03<00:00, 29691.28 examples/s]Running tokenizer on dataset: 100%|██████████| 104743/104743 [00:03<00:00, 28665.53 examples/s]
Running tokenizer on dataset:   0%|          | 0/5463 [00:00<?, ? examples/s]Running tokenizer on dataset:  75%|███████▍  | 4096/5463 [00:00<00:00, 35446.24 examples/s]Running tokenizer on dataset: 100%|██████████| 5463/5463 [00:00<00:00, 21420.00 examples/s]
Generating train split:   0%|          | 0/363846 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 363846/363846 [00:00<00:00, 3683685.07 examples/s]
Generating validation split:   0%|          | 0/40430 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 40430/40430 [00:00<00:00, 3170409.83 examples/s]
Generating test split:   0%|          | 0/390965 [00:00<?, ? examples/s]Generating test split:  95%|█████████▍| 370000/390965 [00:00<00:00, 3682872.46 examples/s]Generating test split: 100%|██████████| 390965/390965 [00:00<00:00, 3655935.65 examples/s]
Running tokenizer on dataset:   0%|          | 0/363846 [00:00<?, ? examples/s]Running tokenizer on dataset:   2%|▏         | 6144/363846 [00:00<00:07, 49733.49 examples/s]Running tokenizer on dataset:   3%|▎         | 12288/363846 [00:00<00:06, 50386.72 examples/s]Running tokenizer on dataset:   5%|▍         | 17408/363846 [00:00<00:09, 36266.31 examples/s]Running tokenizer on dataset:   6%|▋         | 23552/363846 [00:00<00:08, 41021.88 examples/s]Running tokenizer on dataset:   8%|▊         | 29696/363846 [00:00<00:09, 34932.98 examples/s]Running tokenizer on dataset:  10%|▉         | 35840/363846 [00:00<00:08, 39485.35 examples/s]Running tokenizer on dataset:  12%|█▏        | 41984/363846 [00:01<00:07, 42967.67 examples/s]Running tokenizer on dataset:  14%|█▎        | 49152/363846 [00:01<00:08, 37050.16 examples/s]Running tokenizer on dataset:  15%|█▌        | 55296/363846 [00:01<00:07, 39914.89 examples/s]Running tokenizer on dataset:  17%|█▋        | 61440/363846 [00:01<00:08, 35385.87 examples/s]Running tokenizer on dataset:  19%|█▊        | 67584/363846 [00:01<00:07, 38521.91 examples/s]Running tokenizer on dataset:  20%|██        | 73728/363846 [00:01<00:07, 41182.00 examples/s]Running tokenizer on dataset:  22%|██▏       | 79872/363846 [00:02<00:07, 35795.41 examples/s]Running tokenizer on dataset:  24%|██▎       | 86016/363846 [00:02<00:07, 39341.43 examples/s]Running tokenizer on dataset:  25%|██▌       | 92160/363846 [00:02<00:07, 35151.15 examples/s]Running tokenizer on dataset:  27%|██▋       | 98304/363846 [00:02<00:06, 38423.14 examples/s]Running tokenizer on dataset:  29%|██▊       | 104448/363846 [00:02<00:06, 41427.27 examples/s]Running tokenizer on dataset:  31%|███       | 111616/363846 [00:02<00:06, 36711.42 examples/s]Running tokenizer on dataset:  32%|███▏      | 117760/363846 [00:03<00:06, 40209.15 examples/s]Running tokenizer on dataset:  34%|███▍      | 123904/363846 [00:03<00:06, 35459.86 examples/s]Running tokenizer on dataset:  36%|███▌      | 130048/363846 [00:03<00:06, 38594.41 examples/s]Running tokenizer on dataset:  37%|███▋      | 136192/363846 [00:03<00:05, 41520.82 examples/s]Running tokenizer on dataset:  39%|███▉      | 142336/363846 [00:03<00:06, 36297.05 examples/s]Running tokenizer on dataset:  41%|████      | 148480/363846 [00:03<00:05, 39428.08 examples/s]Running tokenizer on dataset:  42%|████▏     | 154624/363846 [00:03<00:04, 42555.05 examples/s]Running tokenizer on dataset:  44%|████▍     | 159744/363846 [00:04<00:05, 36121.53 examples/s]Running tokenizer on dataset:  46%|████▌     | 165888/363846 [00:04<00:04, 39665.68 examples/s]Running tokenizer on dataset:  47%|████▋     | 172032/363846 [00:04<00:05, 35460.28 examples/s]Running tokenizer on dataset:  49%|████▉     | 178176/363846 [00:04<00:04, 39294.35 examples/s]Running tokenizer on dataset:  51%|█████     | 184320/363846 [00:04<00:04, 42577.15 examples/s]Running tokenizer on dataset:  52%|█████▏    | 190464/363846 [00:04<00:04, 37018.13 examples/s]Running tokenizer on dataset:  54%|█████▍    | 196608/363846 [00:05<00:04, 40644.11 examples/s]Running tokenizer on dataset:  56%|█████▌    | 202752/363846 [00:05<00:03, 43576.72 examples/s]Running tokenizer on dataset:  57%|█████▋    | 207872/363846 [00:05<00:04, 37008.06 examples/s]Running tokenizer on dataset:  59%|█████▉    | 214016/363846 [00:05<00:03, 40400.18 examples/s]Running tokenizer on dataset:  61%|██████    | 220160/363846 [00:05<00:04, 35713.93 examples/s]Running tokenizer on dataset:  62%|██████▏   | 226304/363846 [00:05<00:03, 39460.65 examples/s]Running tokenizer on dataset:  64%|██████▍   | 232448/363846 [00:05<00:03, 42718.02 examples/s]Running tokenizer on dataset:  66%|██████▌   | 238592/363846 [00:06<00:03, 37226.94 examples/s]Running tokenizer on dataset:  67%|██████▋   | 244736/363846 [00:06<00:02, 40742.77 examples/s]Running tokenizer on dataset:  69%|██████▉   | 250880/363846 [00:06<00:02, 43587.46 examples/s]Running tokenizer on dataset:  70%|███████   | 256000/363846 [00:06<00:02, 37051.53 examples/s]Running tokenizer on dataset:  72%|███████▏  | 262144/363846 [00:06<00:02, 40756.09 examples/s]Running tokenizer on dataset:  74%|███████▎  | 268288/363846 [00:06<00:02, 35976.16 examples/s]Running tokenizer on dataset:  75%|███████▌  | 274432/363846 [00:07<00:02, 39699.50 examples/s]Running tokenizer on dataset:  77%|███████▋  | 280576/363846 [00:07<00:01, 42765.49 examples/s]Running tokenizer on dataset:  79%|███████▉  | 286720/363846 [00:07<00:02, 37022.79 examples/s]Running tokenizer on dataset:  80%|████████  | 292864/363846 [00:07<00:01, 40658.59 examples/s]Running tokenizer on dataset:  82%|████████▏ | 299008/363846 [00:07<00:01, 43691.88 examples/s]Running tokenizer on dataset:  84%|████████▎ | 304128/363846 [00:07<00:01, 37206.96 examples/s]Running tokenizer on dataset:  85%|████████▌ | 310272/363846 [00:07<00:01, 40881.78 examples/s]Running tokenizer on dataset:  87%|████████▋ | 316416/363846 [00:08<00:01, 35998.81 examples/s]Running tokenizer on dataset:  89%|████████▊ | 322560/363846 [00:08<00:01, 39598.18 examples/s]Running tokenizer on dataset:  90%|█████████ | 328704/363846 [00:08<00:00, 42681.46 examples/s]Running tokenizer on dataset:  92%|█████████▏| 334848/363846 [00:08<00:00, 37109.92 examples/s]Running tokenizer on dataset:  94%|█████████▎| 340992/363846 [00:08<00:00, 40595.03 examples/s]Running tokenizer on dataset:  95%|█████████▌| 347136/363846 [00:08<00:00, 43175.85 examples/s]Running tokenizer on dataset:  97%|█████████▋| 352256/363846 [00:09<00:00, 36758.22 examples/s]Running tokenizer on dataset:  99%|█████████▊| 358400/363846 [00:09<00:00, 40146.84 examples/s]Running tokenizer on dataset: 100%|██████████| 363846/363846 [00:09<00:00, 42351.43 examples/s]Running tokenizer on dataset: 100%|██████████| 363846/363846 [00:09<00:00, 39239.57 examples/s]
Running tokenizer on dataset:   0%|          | 0/40430 [00:00<?, ? examples/s]Running tokenizer on dataset:   5%|▌         | 2048/40430 [00:00<00:02, 14983.37 examples/s]Running tokenizer on dataset:  20%|██        | 8192/40430 [00:00<00:00, 34402.00 examples/s]Running tokenizer on dataset:  35%|███▌      | 14336/40430 [00:00<00:00, 41847.43 examples/s]Running tokenizer on dataset:  51%|█████     | 20480/40430 [00:00<00:00, 34513.24 examples/s]Running tokenizer on dataset:  66%|██████▌   | 26624/40430 [00:00<00:00, 39348.03 examples/s]Running tokenizer on dataset:  81%|████████  | 32768/40430 [00:00<00:00, 34392.21 examples/s]Running tokenizer on dataset:  96%|█████████▌| 38912/40430 [00:01<00:00, 38600.85 examples/s]Running tokenizer on dataset: 100%|██████████| 40430/40430 [00:01<00:00, 36980.06 examples/s]
Generating train split:   0%|          | 0/2490 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 2490/2490 [00:00<00:00, 543977.13 examples/s]
Generating validation split:   0%|          | 0/277 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 277/277 [00:00<00:00, 178631.95 examples/s]
Generating test split:   0%|          | 0/3000 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 3000/3000 [00:00<00:00, 748270.22 examples/s]
Running tokenizer on dataset:   0%|          | 0/2490 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 2490/2490 [00:00<00:00, 27196.38 examples/s]
Running tokenizer on dataset:   0%|          | 0/277 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 277/277 [00:00<00:00, 22440.26 examples/s]
Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 3844027.16 examples/s]
Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 531371.94 examples/s]
Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 780166.25 examples/s]
Running tokenizer on dataset:   0%|          | 0/67349 [00:00<?, ? examples/s]Running tokenizer on dataset:   8%|▊         | 5120/67349 [00:00<00:01, 32874.34 examples/s]Running tokenizer on dataset:  23%|██▎       | 15360/67349 [00:00<00:00, 60361.32 examples/s]Running tokenizer on dataset:  40%|███▉      | 26624/67349 [00:00<00:00, 54161.53 examples/s]Running tokenizer on dataset:  55%|█████▍    | 36864/67349 [00:00<00:00, 50796.20 examples/s]Running tokenizer on dataset:  68%|██████▊   | 46080/67349 [00:00<00:00, 58614.82 examples/s]Running tokenizer on dataset:  85%|████████▌ | 57344/67349 [00:01<00:00, 55069.08 examples/s]Running tokenizer on dataset:  97%|█████████▋| 65536/67349 [00:01<00:00, 60128.71 examples/s]Running tokenizer on dataset: 100%|██████████| 67349/67349 [00:01<00:00, 52596.83 examples/s]
Running tokenizer on dataset:   0%|          | 0/872 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 872/872 [00:00<00:00, 52450.60 examples/s]
Generating train split:   0%|          | 0/5749 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 5749/5749 [00:00<00:00, 1562130.97 examples/s]
Generating validation split:   0%|          | 0/1500 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 1500/1500 [00:00<00:00, 789887.76 examples/s]
Generating test split:   0%|          | 0/1379 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 1379/1379 [00:00<00:00, 756367.88 examples/s]
Running tokenizer on dataset:   0%|          | 0/5749 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 5749/5749 [00:00<00:00, 51978.54 examples/s]Running tokenizer on dataset: 100%|██████████| 5749/5749 [00:00<00:00, 51305.02 examples/s]
Running tokenizer on dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]Running tokenizer on dataset: 100%|██████████| 1500/1500 [00:00<00:00, 44602.54 examples/s]
Downloading builder script: 0.00B [00:00, ?B/s]Downloading builder script: 5.75kB [00:00, 40.3MB/s]
slurmstepd: error: *** JOB 3921331 ON ramses16307 CANCELLED AT 2026-01-18T22:29:00 DUE TO TIME LIMIT ***
